# -*- coding: utf-8 -*-
"""remoteHacking_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOgxDtYN6qQch9KRAHX1CfIyQV_gHkJl
"""

!pip install duration

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from duration import (
    to_iso8601,
    to_seconds,
    to_timedelta,
    to_tuple,
)
from google.colab import drive
import psycopg2 as ps
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""*Importing the 3 datasets*"""

drive.mount('/content/drive')

df=pd.read_csv('/content/drive/My Drive/Remote Hacking/Status_2016-01-01_-_2020-03-01.csv',sep=';')
s1617=pd.read_csv('/content/drive/My Drive/Remote Hacking/Turbine_Data_2016-01-01_-_2017-12-31.csv', sep=';', decimal=",")
s1820=pd.read_csv('/content/drive/My Drive/Remote Hacking/Turbine_Data_2018-01-01_-_2020-03-01.csv', sep=';', decimal=".")

s1820.info()

"""*Sensor Data Cleaning*"""

sensor=pd.concat([s1617, s1820], ignore_index=True)

# Turning the index into a datetime index
sensor.index=pd.to_datetime(sensor['# Date and time'])

cols=sensor.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if "PA18" in cols[i]:
        newcols.append(cols[i])

pa18=sensor[newcols]

pa18.columns.tolist()[1][7:]

cols=pa18.columns.tolist()
newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa18.columns=newcols

pa18.insert(0, "Turbine", "PA18")

pa18

pa18.info()

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa18.dropna(axis=1, how='all', inplace=True) # thresh=200000

#Checking for missing values
total = pa18.isnull().sum().sort_values(ascending=False)
percent_1 = pa18.isnull().sum()/pa18.isnull().count()*100
percent_2 = (round(percent_1, 2)).sort_values(ascending=False)
missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'], sort=False)
missing_data.head(30)

"""*Preparing the error dataset*"""

df["Timestamp start"]=pd.to_datetime(df["Timestamp start"])
#df["Timestamp end"]=pd.to_datetime(df["Timestamp end"])
df.index=pd.to_datetime(df['Timestamp start'])

errors_pa18=df[(df["Turbine"]=="PA18")]

errors_pa18

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

"""*Joining the datasets*"""

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data=pd.merge(left=pa18, right=errors_pa18, how="left", left_index=True, right_index=True)

data[(data["Timestamp start"].notnull())]

"""**DATA EXPLORATION**"""

errors_pa18.groupby(['Code']).sum().sort_values(by="Duration_Minutes", ascending=False).head(50)

errors_pa18.groupby(['Code']).sum().sort_values(by="Duration_Minutes", ascending=False).head(30).plot(kind='bar',figsize=(20,5))

data.describe()

data["Controller VCP temperature (Â°C)"].plot(figsize=(20,5))

error_dict = errors_pa18.drop_duplicates(subset=['Code', 'Message'])

unique_Codes=set(errors_pa18.Code)
 unique_Messages=set(errors_pa18.Message)

error_counter={} #empty dictionary, codes will become keys
for i in unique_Codes:
  if data.Duration_Minutes[data.Code==i].mean()!=0: #no warnings, just error codes
    lst=set(errors_pa18.Message[errors_pa18.Code==i]) #list of unique messages per code
    error_counter[i]=[len(data.Code[data.Code==i]), #count
                      round(data.Duration_Minutes[data.Code==i].mean(),2), #duration in minutes
                      len(data.Code[data.Code==i])*round(data.Duration_Minutes[data.Code==i].mean(),2), #code times duration
                      lst] # list from above
error_counts=pd.DataFrame(error_counter.values(), #turn dictionary into dataframe
                          columns=["Count","Avg_Duration","Count times Duration","Messages"],
                          index=error_counter.keys())
error_counts.sort_values(by="Count times Duration",ascending=False).head(15) #top 15 by Count times Duration

error_counts[(error_counts.Avg_Duration>60)&(error_counts.Count>5)].sort_values(by="Count",ascending=False)

messages_per_Code={}
for i in unique_Codes:
  lst=set(errors_pa18.Message[errors_pa18.Code==i])
  messages_per_Code[i]=lst
messages_per_Code[159],messages_per_Code[919],messages_per_Code[311],

"""**DATA CLEANING**

*Inserting Null-Values & Aligning Data Types*
"""

champ_cols={}
for col in data.columns:
  try:
    champ_cols[col]=data[col].value_counts()["#CHAMP!"]
  except:
    champ_cols[col]=0
champ_cols

data=data.replace('#CHAMP!',np.NaN)

data['Lost Production Total (kWh)'].value_counts(), data['Power (kW)'].value_counts(), data['Reactive power (kvar)'].value_counts()

change_dtype=["Lost Production Total (kWh)","Power (kW)","Reactive power (kvar)"]
for col in change_dtype:
  data[col] = data[col].astype("float")

data.info()

"""*   no more #CHAMP!-values in the three relevant columns
*   all data-types numerical (either float or integer)

**DATA AUGMENTATION**

*Fill Missing Values with Means & Standardize*
"""

to_standardize=[]
for col in data.columns:
  if data[col].dtype=="float" and col!="Code":
    to_standardize.append(col)

#data[to_standardize]=data[to_standardize].fillna(data[to_standardize].mean())

#scaler=StandardScaler()
#data[to_standardize] = scaler.fit_transform(data[to_standardize])

"""*Creating the Countdown in Windows of 10 minutes*"""

countdown=[]
counter=0
for Code,Status in zip(data.Code[::-1],data.Status[::-1]):
    if Code>0 and Code!=900 and Status=="Stop":
      counter=0
      countdown.append(counter)
    else:
      counter=counter+10
      countdown.append(counter)
data["Countdown"]=countdown[::-1]

data=data[data.Code!=900]

"""*Creating Labels*"""

status_label=[]
for i in range(len(data)):
    if pd.isnull(data.Status[i]):
        status_label.append("No Problem")
    else:
        status_label.append(data.Status[i])
data["status_label"] = status_label

code_label=[]
for i in range(len(data)):
    if pd.isnull(data.Code[i]):
        code_label.append(0)
    else:
        code_label.append(data.Code[i])

for i in range(len(code_label)):
    if code_label[i] != 0:
        code_label[i] =1
data["code_label"] = code_label

data.to_csv("/content/drive/My Drive/Remote Hacking/data_for_Z.csv")

"""*Filter out irrelevant Columns*"""

data=data.drop(["Turbine","Status","Code","Message","Comment","Timestamp start","Timestamp end","Duration","Duration_Minutes"],axis=1)

#data.to_csv("/content/drive/My Drive/Remote Hacking/pa18_2.csv")

"""**MODELING**

*Sliding Window Neural Net - Classification*
"""

w1 = 2880
w0 = 1440
data['NN_label1'] = np.where(data.Countdown <= w1, 1, 0 )
data['NN_label2'] = data.NN_label1
data.loc[data.Countdown <= w0, 'NN_label2'] = 2

data.NN_label1.value_counts()

data.NN_label1[data.ID==2].value_counts()

ID=[]
counter=1
for c in data.Countdown:
  if c!=0:
    ID.append(counter)
  if c==0:
    ID.append(counter)
    counter=counter+1
data.insert(0,"ID",ID)

data.insert(0,"Index",range(0,len(data)))

index_ID={}
for id in data.ID.unique():
  index_ID[id]=(data.Index[data.ID == id].tolist()[::-1][0])

index_ID_values=[]
for i in index_ID.values():
  index_ID_values.append(i)

windows=[]
for id in data.ID.unique():
  windows.append(len(data[data.ID==id]))

windows

ID_windows={}
for id,w in zip(data.ID.unique(),windows):
  ID_windows[id]=w

print(ID_windows)

week_instances=[]
for key in ID_windows.keys():
  if ID_windows[key]>2016:
    week_instances.append(key)

len(week_instances)

data=data.loc[data.ID.isin(week_instances)]

#data.to_csv("/content/drive/My Drive/Remote Hacking/data_with_week_instances.csv")

splits=[]
for split in range(1,4+1):
    split_at_ID=data.ID[int(round(len(data)*split/4,0))-1]
    splits.append((index_ID[split_at_ID])+1)
splits

data_1 = data[0:splits[0]]
data_2 = data[splits[0]:splits[1]]
data_3 = data[splits[1]:splits[2]]
data_4 = data[splits[2]:]

def gen_sequence(id_df, seq_length, seq_cols):
    data_array = id_df[seq_cols].values
    num_elements = data_array.shape[0]
    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):
        yield data_array[start:stop, :]

a = data[data.ID==18]nce_cols].values
a.shape[0]

data.ID.unique()

sequence_length = 1008

sequence_cols = data.columns.to_list()
sequence_cols = sequence_cols[2:49]

#seq_gen = (list(gen_sequence(data[data.ID==2], sequence_length, sequence_cols)))
seq_gen = (list(gen_sequence(data[data.ID==id], sequence_length, sequence_cols)) 
           for id in data.ID.unique())
'''
seq_gen_1 = (list(gen_sequence(data_1[data_1.ID==id], sequence_length, sequence_cols)) 
           for id in data_1.ID.unique())
seq_gen_2 = (list(gen_sequence(data_2[data_2.ID==id], sequence_length, sequence_cols)) 
           for id in data_2.ID.unique())
seq_gen_3 = (list(gen_sequence(data_3[data_3.ID==id], sequence_length, sequence_cols)) 
           for id in data_3.ID.unique())
seq_gen_4 = (list(gen_sequence(data_4[data_4.ID==id], sequence_length, sequence_cols)) 
           for id in data_.ID.unique())
           '''

list(seq_gen)

seq_array = np.concatenate(list(seq_gen)).astype(np.float32)
seq_array.shape

'''
seq_array=[]
for id in range(len(data['ID'].unique())):
  arr=gen_sequence(data[data.ID==id], sequence_length, sequence_cols)
  seq_array.append(arr)
'''

len(seq_array)

data[150:180]

"""*Classification Model*"""

#data=pd.read_csv('/content/drive/My Drive/Remote Hacking/preprocessed_data.csv')
data.head()

X=data.iloc[:,0:49]
y=data.iloc[:,50:51]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33 , stratify=y, random_state = 0)

from imblearn.over_sampling import SMOTE
from collections import Counter
from imblearn.under_sampling import ClusterCentroids
from imblearn.under_sampling import RandomUnderSampler
# Over sampling followed by undersampling
print('Original dataset shape %s' % Counter(y_train))
sm=SMOTE(random_state=42, sampling_strategy=0.01)
X_res, y_res=sm.fit_resample(X_train,y_train)
cc = RandomUnderSampler(random_state=42, sampling_strategy=0.9)
X_res, y_res = cc.fit_resample(X_res,y_res)
print('Resampled dataset shape %s' % Counter(y_res))

from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
xg= xgb.XGBClassifier(objective='binary:logistic', max_depth=3, learning_rate=0.5, verbosity=1,\
                      gamma=0, n_estimators=20, subsample=0.7, colsample_bytree=0.5, early_stopping_rounds=5)
xg.fit(X_train, y_train)
predictions=xg.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

from sklearn.metrics import confusion_matrix
confusion_matrix(predictions,y_test)

from sklearn.metrics import plot_roc_curve
ax = plt.gca()
test_rf = plot_roc_curve(xg, X_test, y_test,ax=ax, alpha=0.8, name="test")
train_rf = plot_roc_curve(xg, X_train, y_train,ax=ax, alpha=0.8, name="train")
plt.show()

print(confusion_matrix(predictions,y_test))

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 50, random_state = 42, max_depth=5)
rf.fit(X_train, y_train)
predictions = rf.predict(X_test)
print(classification_report(y_test,predictions))
ax = plt.gca()
test_rf = plot_roc_curve(rf, X_test, y_test,ax=ax, alpha=0.8, name="test")
train_rf = plot_roc_curve(rf, X_train, y_train,ax=ax, alpha=0.8, name="train")
plt.show()

print(confusion_matrix(predictions,y_test))

!pip install duration

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from duration import (
    to_iso8601,
    to_seconds,
    to_timedelta,
    to_tuple,
)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter
from imblearn.under_sampling import ClusterCentroids
from imblearn.under_sampling import RandomUnderSampler
#import psycopg2 as ps
#from google.colab import drive


*Importing the 3 datasets*

#drive.mount('/content/drive')

os.chdir("/Users/andreasabia/Documents/ESCP/Hackathon")
df=pd.read_csv('./Status_2016-01-01_-_2020-03-01.csv',sep=';')
s1617=pd.read_csv('./Turbine_Data_2016-01-01_-_2017-12-31.csv', sep=';', decimal=',')
s1820=pd.read_csv('./Turbine_Data_2018-01-01_-_2020-03-01.csv', sep=';',decimal='.')

df=pd.read_csv('/content/drive/My Drive/Remote Hacking/Status_2016-01-01_-_2020-03-01.csv',sep=';')
s1617=pd.read_csv('/content/drive/My Drive/Remote Hacking/Turbine_Data_2016-01-01_-_2017-12-31.csv', sep=';', decimal=",")
s1820=pd.read_csv('/content/drive/My Drive/Remote Hacking/Turbine_Data_2018-01-01_-_2020-03-01.csv', sep=';', decimal=".")

*Sensor Data Cleaning*

sensor=pd.concat([s1617, s1820], ignore_index=True)

# Turning the index into a datetime index
sensor.index=pd.to_datetime(sensor['# Date and time'])

cols=sensor.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if "PA18" in cols[i]:
        newcols.append(cols[i])

pa18=sensor[newcols]

cols=pa18.columns.tolist()
newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa18.columns=newcols

pa18.insert(0, "Turbine", "PA18")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa18.dropna(axis=1, how='all', inplace=True) # thresh=200000

cols=sensor.columns.tolist()

newcols=[]

for i in range(len(cols)):
    if "PA20" in cols[i]:
        newcols.append(cols[i])

pa20=sensor[newcols]

cols=pa20.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa20.columns=newcols

pa20.insert(0, "Turbine", "PA20")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa20.dropna(axis=1, how='all', inplace=True) # thresh=200000

cols=sensor.columns.tolist()

newcols=[]

for i in range(len(cols)):
    if "PA21" in cols[i]:
        newcols.append(cols[i])

pa21=sensor[newcols]

cols=pa21.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa21.columns=newcols

pa21.insert(0, "Turbine", "PA21")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa21.dropna(axis=1, how='all', inplace=True) # thresh=200000

cols=sensor.columns.tolist()

newcols=[]

for i in range(len(cols)):
    if "PA22" in cols[i]:
        newcols.append(cols[i])

pa22=sensor[newcols]

cols=pa22.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa22.columns=newcols

pa22.insert(0, "Turbine", "PA22")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa22.dropna(axis=1, how='all', inplace=True) # thresh=200000

cols=sensor.columns.tolist()

newcols=[]

for i in range(len(cols)):
    if "PA23" in cols[i]:
        newcols.append(cols[i])

pa23=sensor[newcols]

cols=pa23.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa23.columns=newcols

pa23.insert(0, "Turbine", "PA23")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa23.dropna(axis=1, how='all', inplace=True) # thresh=200000

cols=sensor.columns.tolist()

newcols=[]

for i in range(len(cols)):
    if "PA24" in cols[i]:
        newcols.append(cols[i])

pa24=sensor[newcols]

cols=pa24.columns.tolist()

newcols=[]
for i in range(len(cols)):
    if '# Date and time' not in cols[i]:
        newcols.append(cols[i][7:])

pa24.columns=newcols

pa24.insert(0, "Turbine", "PA24")

# Dropping Rows with Over 50% NAs
# Some columns have been added from 2018 onwards, so maybe we can focus only on that period if we want to use those?
pa24.dropna(axis=1, how='all', inplace=True) # thresh=200000

*Preparing the error dataset*

df["Timestamp start"]=pd.to_datetime(df["Timestamp start"])
#df["Timestamp end"]=pd.to_datetime(df["Timestamp end"])
df.index=pd.to_datetime(df['Timestamp start'])

errors_pa18=df[(df["Turbine"]=="PA18")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index


errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data=pd.merge(left=pa18, right=errors_pa18, how="left", left_index=True, right_index=True)

code_label=[]
for i in range(len(data)):
    if pd.isnull(data.Code[i]):
        code_label.append(0)
    else:
        code_label.append(1)

data["code_label"] = code_label

sum(data["code_label"])

errors_pa18=df[(df["Turbine"]=="PA20")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data20=pd.merge(left=pa20, right=errors_pa18, how="left", left_index=True, right_index=True)

code_label=[]
for i in range(len(data20)):
    if pd.isnull(data20.Code[i]):
        code_label.append(0)
    else:
        code_label.append(1)

data20["code_label"] = code_label

errors_pa18=df[(df["Turbine"]=="PA21")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data21=pd.merge(left=pa21, right=errors_pa18, how="left", left_index=True, right_index=True)

errors_pa18=df[(df["Turbine"]=="PA22")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data22=pd.merge(left=pa22, right=errors_pa18, how="left", left_index=True, right_index=True)

code_label=[]
for i in range(len(data22)):
    if pd.isnull(data22.Code[i]):
        code_label.append(0)
    else:
        code_label.append(1)

data22["code_label"] = code_label

errors_pa18=df[(df["Turbine"]=="PA23")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data23=pd.merge(left=pa23, right=errors_pa18, how="left", left_index=True, right_index=True)

code_label=[]
for i in range(len(data23)):
    if pd.isnull(data23.Code[i]):
        code_label.append(0)
    else:
        code_label.append(1)

data23["code_label"] = code_label

errors_pa18=df[(df["Turbine"]=="PA24")]

for i in range(len(errors_pa18)):
    if errors_pa18["Duration"][i]== "-":
        errors_pa18["Duration"][i]=0

dur=[]
for i in range(len(errors_pa18)):
    td = to_seconds(errors_pa18["Duration"][i], strict=False)
    td=td/60
    dur.append(td)

errors_pa18["Duration_Minutes"]=dur

tm=errors_pa18.index[1]
tm = tm - datetime.timedelta(minutes=tm.minute % 10,
                             seconds=tm.second,
                             microseconds=tm.microsecond)
tm

new_index=[]
for i in range(len(errors_pa18)):
    tm = errors_pa18.index[i] - datetime.timedelta(minutes=errors_pa18.index[i].minute % 10,
                             seconds=errors_pa18.index[i].second,
                             microseconds=errors_pa18.index[i].microsecond)
    new_index.append(tm)

errors_pa18.index=new_index

errors_pa18.drop(["Turbine"], axis=1, inplace=True)

data24=pd.merge(left=pa24, right=errors_pa18, how="left", left_index=True, right_index=True)

code_label=[]
for i in range(len(data24)):
    if pd.isnull(data24.Code[i]):
        code_label.append(0)
    else:
        code_label.append(1)

data24["code_label"] = code_label

finaldata=data.append(data20)

finaldata=finaldata.append(data22)

finaldata=finaldata.append(data23)

finaldata=finaldata.append(data24)

data=finaldata


**DATA EXPLORATION**



**DATA CLEANING**

*Inserting Null-Values & Aligning Data Types*

champ_cols={}
for col in data.columns:
  try:
    champ_cols[col]=data[col].value_counts()["#CHAMP!"]
  except:
    champ_cols[col]=0
champ_cols

data=data.replace('#CHAMP!',np.NaN)

change_dtype=["Lost Production Total (kWh)","Power (kW)","Reactive power (kvar)"]
for col in change_dtype:
  data[col] = data[col].astype("float")

*   no more #CHAMP!-values in the three relevant columns
*   all data-types numerical (either float or integer)



**DATA AUGMENTATION**

*Fill Missing Values with Means & Standardize*

to_standardize=[]
for col in data.columns:
  if data[col].dtype=="float" and col!="Code":
    to_standardize.append(col)

data[to_standardize]=data[to_standardize].fillna(data[to_standardize].mean())

scaler=StandardScaler()
data[to_standardize] = scaler.fit_transform(data[to_standardize])

*Creating the Countdown in Windows of 10 minutes*

countdown=[]
counter=0
for Code,Status in zip(data.Code[::-1],data.Status[::-1]):
    if Code>0 and Code!=900 and Status=="Stop":
      counter=0
      countdown.append(counter)
    else:
      counter=counter+10
      countdown.append(counter)
data["Countdown"]=countdown[::-1]

data=data[data.Code!=900]