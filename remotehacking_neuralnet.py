# -*- coding: utf-8 -*-
"""remoteHacking_NeuralNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dACbRZzDxW0Nocbj3GAsmcAw0D3vK3QH
"""

!pip install duration

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
from duration import (
    to_iso8601,
    to_seconds,
    to_timedelta,
    to_tuple,
)
from google.colab import drive
import psycopg2 as ps
from random import sample

drive.mount('/content/drive')

data=pd.read_csv('/content/drive/My Drive/Remote Hacking/NN_data_v2.csv',sep=";").iloc[:,1:]

#train/test-split into parts of 66% and 33% respectively

test_IDs = sample(set(data.ID.unique()), int(round(len(data.ID.unique())/3,0)))

train_IDs = []
for i in data.ID.unique():
  if i not in test_IDs:
    train_IDs.append(i)

train_data = data[data.ID.isin(train_IDs)]
test_data = data[data.ID.isin(test_IDs)]

def gen_sequence(id_df, seq_length, seq_cols):
    data_array = id_df[seq_cols].values
    num_elements = data_array.shape[0]
    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):
      yield data_array[start:stop, :]

sequence_length = 144

sequence_cols = train_data.columns.to_list()
sequence_cols = sequence_cols[2:len(sequence_cols)-2]

seq_gen = (list(gen_sequence(train_data[train_data.ID==id], sequence_length, sequence_cols)) 
           for id in train_data.ID.unique())

seq_array = np.concatenate(list(seq_gen)).astype(np.float32)
seq_array.shape

def gen_labels(id_df, seq_length, label):
    data_array = id_df[label].values
    num_elements = data_array.shape[0]
    return data_array[seq_length:]

label_gen = [gen_labels(train_data[train_data.ID==id], sequence_length, ['NN_label']) 
             for id in train_data.ID.unique()]
label_array = np.concatenate(label_gen).astype(np.float32)
label_array.shape

np.unique(label_array,return_counts=True)

test_gen = (list(gen_sequence(test_data[test_data.ID==id], sequence_length, sequence_cols)) 
           for id in test_data.ID.unique())

test_array = np.concatenate(list(test_gen)).astype(np.float32)
test_array.shape

test_label_gen = [gen_labels(test_data[test_data.ID==id], sequence_length, ['NN_label']) 
             for id in test_data.ID.unique()]
test_label_array = np.concatenate(test_label_gen).astype(np.float32)
test_label_array.shape

np.unique(test_label_array,return_counts=True)

"""**CLASSIFICATION NEURAL NET**"""

import keras
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Activation
import keras.backend as K

"""*Model Building*"""

def get_f1(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

nb_features = seq_array.shape[2]
nb_out = label_array.shape[1]

model = Sequential()
model.add(LSTM(input_shape=(sequence_length, nb_features), units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=nb_out, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=["accuracy"])

print(model.summary())

"""*Model Training*"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(seq_array, label_array, epochs=2, batch_size=1000, validation_split=0.1, verbose=1,
#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')])



"""*Model Results (Train)*"""

scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)
print('Accurracy: {}'.format(scores[1]))

scores

y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)
y_true = label_array
print('Confusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels')
cm = confusion_matrix(y_true, y_pred)
print (cm)

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
print( 'precision = ', precision, '\n', 'recall = ', recall)

"""*Model Results (Test)*"""

scores_test = model.evaluate(test_array, test_label_array, verbose=2)
print('Accurracy: {}'.format(scores_test[1]))

y_pred_test = model.predict_classes(test_array)
y_true_test = test_label_array
print('Confusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels')
cm = confusion_matrix(y_true_test, y_pred_test)
print (cm)

precision_test = precision_score(y_true_test, y_pred_test)
recall_test = recall_score(y_true_test, y_pred_test)
f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)
print( 'Precision: ', precision_test, '\n', 'Recall: ', recall_test,'\n', 'F1-score:', f1_test )

"""**REGRESSION NEURAL NET**"""

from keras.optimizers import Adam, RMSprop
from keras.layers import Input, Reshape, Dropout
from keras import Model

nb_features = seq_array.shape[2]
nb_out = label_array_REG.shape[1]

model = Sequential()
model.add(LSTM(
         units=100,
         return_sequences=True,
         input_shape=(sequence_length, nb_features)))
model.add(Dropout(0.4))
model.add(LSTM(
          units=100,
          return_sequences=False))
model.add(Dropout(0.4))
model.add(Dense(units=1, activation='relu'))
model.add(Activation("relu"))
model.compile(loss="mse", optimizer="rmsprop", metrics=['mse'])

print(model.summary())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # fit the network
# hist = model.fit(seq_array, label_array_REG, epochs=3, batch_size=1000, validation_split=0.1, verbose=1,
#           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')])

plt.plot(hist.history['mean_squared_error'])
plt.plot(hist.history['val_mean_squared_error'])
plt.title('MSE')
plt.ylabel('Mean Squared Error')
plt.xlabel('# Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# training metrics
scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)
print('MSE: {}'.format(scores[1]))

